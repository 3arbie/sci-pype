{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the IRIS XGB Models and Analysis from Redis Labs Cloud\n",
    "\n",
    "This notebook demonstrates how to extract the machine learning Models + Analysis from the Redis Labs Cloud (https://redislabs.com/redis-cloud) cache endpoint named \"**CACHE**\" and saved locally as a compressed, string artifact file (Pickle + zlib compression). Once the file is saved, it is uploaded to the configured S3 Bucket for archiving and sharing.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Extract the IRIS XGB regressor models from the Redis Labs Cloud **CACHE** endpoint. After extraction, compile a manifest for defining a cache mapping for all the Models + their respective Analysis. Once cached, the Models can be extract and shared + deployed on other Sci-pype instances by using something like this notebook or the command-line versions.\n",
    "\n",
    "### Command-line Versions\n",
    "\n",
    "I built this notebook from the extractor examples:\n",
    "\n",
    "https://github.com/jay-johnson/sci-pype/tree/master/bins/ml/extractors/rl_extract_and_upload_iris_regressor.py\n",
    "https://github.com/jay-johnson/sci-pype/tree/master/bins/ml/extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "### 1) Extract the IRIS XGB Regressor Models + Analysis from the Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup the Sci-pype environment\n",
    "import sys, os\n",
    "\n",
    "# Only Redis Labs is needed for this notebook:\n",
    "os.environ[\"ENV_DEPLOYMENT_TYPE\"] = \"RedisLabs\"\n",
    "\n",
    "# Load the Sci-pype PyCore as a named-object called \"core\" and environment variables\n",
    "from src.common.load_ipython_env import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Setup the Request\n",
    "\n",
    "Extract the Models from the Cache with this request and upload them object files to the configured S3 Bucket.\n",
    "\n",
    "Please make sure the environment variables are set correctly and the S3 Bucket exists:\n",
    "\n",
    "   ```\n",
    "   ENV_AWS_KEY=<AWS API Key>\n",
    "   ENV_AWS_SECRET=<AWS API Secret>\n",
    "   ```\n",
    "\n",
    "For docker containers make sure to set these keys in the correct Jupyter env file and restart the container:\n",
    "\n",
    "   ```\n",
    "   <repo base dir>/justredis/redis-labs.env\n",
    "   <repo base dir>/local/jupyter.env\n",
    "   <repo base dir>/test/jupyter.env\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- What's the dataset name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds_name             = \"iris_regressor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Where is the downloaded file getting stored?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir            = str(os.getenv(\"ENV_DATA_DST_DIR\", \"/opt/work/data/dst\"))\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir, 0777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What's the S3 Location (Unique Bucket Name + Key)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3_bucket           = \"unique-bucket-name-for-datasets\" # name this something under your AWS Account (This might be open to the public in the future...stay tuned)\n",
    "s3_key              = \"dataset_\" + core.to_upper(ds_name) + \".cache.pickle.zlib\"\n",
    "s3_loc              = str(s3_bucket) + \":\" + str(s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Build and Run the Extract + Upload Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cache_req           = {\n",
    "                        \"RAName\"        : \"CACHE\",    # Redis endpoint name holding the models\n",
    "                        \"DSName\"        : str(ds_name), # Dataset name for pulling out of the cache\n",
    "                        \"S3Loc\"         : str(s3_loc),  # S3 location to store the model file\n",
    "                        \"DeleteAfter\"   : False,        # Optional delete after upload\n",
    "                        \"SaveDir\"       : data_dir,     # Optional dir to save the model file - default is ENV_DATA_DST_DIR\n",
    "                        \"TrackingID\"    : \"\"            # Future support for using the tracking id\n",
    "                    }\n",
    "\n",
    "upload_results      = core.ml_upload_cached_dataset_to_s3(cache_req, core.get_rds(), core.get_dbs(), debug)\n",
    "if upload_results[\"Status\"] == \"SUCCESS\":\n",
    "    lg(\"Done Uploading Model and Analysis DSName(\" + str(ds_name) + \") S3Loc(\" + str(cache_req[\"S3Loc\"]) + \")\", 6)\n",
    "else:\n",
    "    lg(\"\", 6)\n",
    "    lg(\"ERROR: Failed Upload Model and Analysis Caches as file for DSName(\" + str(ds_name) + \")\", 6)\n",
    "    lg(upload_results[\"Error\"], 6)\n",
    "    lg(\"\", 6)\n",
    "    sys.exit(1)\n",
    "# end of if extract + upload worked\n",
    "\n",
    "lg(\"\", 6)\n",
    "lg(\"Extract and Upload Completed\", 5)\n",
    "lg(\"\", 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automation with Lambda - Coming Soon\n",
    "\n",
    "Native lambda uploading support will be added in the future. Packaging and functionality still need to be figured out. For now, you can extend the command line versions for the extractors below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "Now that the XGB models are archived as an artifact on S3, you can run the following notebooks to checkout how the Sci-pype workflow continues using this data science artifact:\n",
    "\n",
    "1. [Build, Train and Cache the XGB Models for the IRIS Dataset on Redis Labs Cloud](./ML-IRIS-Redis-Labs-Cache-XGB-Regressors.ipynb)\n",
    "1. [This Notebook - Extract from the Machine Learning data store and archive the artifact on S3](./ML-IRIS-Redis-Labs-Extract-From-Cache.ipynb)\n",
    "1. [Import the artifacts from S3 and deploy them to the Machine Learning data store](./ML-IRIS-Redis-Labs-Import-From-S3.ipynb)\n",
    "1. [Make new Predictions using the cached XGB Models](./ML-IRIS-Redis-Labs-Predict-From-Cached-XGB.ipynb)\n",
    "\n",
    "#### Command-line Versions\n",
    "\n",
    "I built this notebook from the extractor examples:\n",
    "\n",
    "https://github.com/jay-johnson/sci-pype/tree/master/bins/ml/extractors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
